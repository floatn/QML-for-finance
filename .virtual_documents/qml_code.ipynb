





import pandas as pd
pf_df = pd.read_csv('./datasets/all_stocks_5yr.csv')


pf_df.info()


pf_df.head()


pf_df.describe()


# Уберем из списка стоки с NA данными
pf_df.drop(pf_df.loc[pf_df['open'].isna(), 'Name'].isin(pf_df['Name']).index, inplace=True)
pf_df.drop(pf_df.loc[pf_df['high'].isna(), 'Name'].isin(pf_df['Name']).index, inplace=True)
pf_df.drop(pf_df.loc[pf_df['low'].isna(), 'Name'].isin(pf_df['Name']).index, inplace=True)


# Преобразование типов
pf_df = pf_df.convert_dtypes()


pf_df.info()


pf_df['date'] = pd.to_datetime(pf_df['date'])
pf_df.info()


# Группы выбора на графике
import matplotlib.pyplot as plt
import seaborn as sns

high_max = pf_df[['high', 'Name']].groupby(['Name']).max()

_, ax = plt.subplots(figsize=(12, 6))
sns.histplot(data=high_max, bins=150, ax=ax)
plt.axvline(x=30, color='red', linestyle='--')
plt.axvline(x=100, color='orange', linestyle='--')
#TODO: use plotly or add plot without ouliers


pf_df1 = high_max[high_max['high'] <= 30].sample(5)
pf_df2 = high_max[high_max['high'].between(30, 100, inclusive='right')].sample(5)
pf_df3 = high_max[high_max['high'] > 100].sample(10)
#pf_df3
#pf_df[pf_df['high']high_max < 30]
#high_max[high_max['high'] < 30].sample(5)
pf_df[pf_df['Name'].isin(pf_df1.index)]['date'].value_counts()


# Давайте узнаем как много организаций не имеют данных на полный период времени
pf_df[['date', 'Name']].groupby(['Name']).count().plot.hist()


# Попробуем использовать для исследования только полные данные.
dates = pf_df['date'].sort_values().unique()
pf_df = pf_df.groupby(['Name']).filter(lambda x: x['date'].count() == len(dates))
high_max = pf_df[['high', 'Name']].groupby(['Name']).max()
pf_df1 = high_max[high_max['high'] <= 30].sample(2, random_state=1)
pf_df2 = high_max[high_max['high'].between(30, 100, inclusive='right')].sample(2, random_state=1)
pf_df3 = high_max[high_max['high'] > 100].sample(6, random_state=1)
pf20_df = pf_df[pf_df['Name'].isin(pf_df1.index)]
pf20_df['class'] = 'C'
pf20_df = pd.concat([pf20_df, pf_df[pf_df['Name'].isin(pf_df2.index)]], ignore_index=True).fillna('B')
pf20_df = pd.concat([pf20_df, pf_df[pf_df['Name'].isin(pf_df3.index)]], ignore_index=True).fillna('A')


fig, ax = plt.subplots(3, 1, figsize=(12, 12))
sns.lineplot(data=pf20_df[pf20_df['class'] == 'A'], x='date', y='close', hue='Name', ax=ax[0]).set(xlabel=None, ylabel=None, title='class A')
sns.lineplot(data=pf20_df[pf20_df['class'] == 'B'], x='date', y='close', hue='Name', ax=ax[1]).set(xlabel=None, ylabel=None, title='class B')
sns.lineplot(data=pf20_df[pf20_df['class'] == 'C'], x='date', y='close', hue='Name', ax=ax[2]).set(ylabel=None, title='class C')
fig.suptitle('Closing prices, $B', fontsize=16, y=0.925)
#g = sns.FacetGrid(pf20_df.sort_values('class'), row='class', aspect=4.)
#g.map(sns.lineplot, 'date', 'high', 'Name')
#g.add_legend()


#date80 = dates[int(0.8*len(dates))]
date_test  = max(dates) - pd.Timedelta(365, 'D')
pf20train_df = pf20_df[pf20_df['date'] <= date_test]
pf20test_df = pf20_df[pf20_df['date'] > date_test]
len(pf20train_df) + len(pf20test_df) == len(pf20_df)


pf20train_df = pf20train_df[['date', 'close', 'Name']].pivot(index='date', columns='Name', values='close').sort_values('date')
pf20test_df = pf20test_df[['date', 'close', 'Name']].pivot(index='date', columns='Name', values='close').sort_values('date')


pf20train_df
pf20_df['date'].value_counts()


pd.Timestamp('2018-02-07') - pd.Timestamp('2013-02-08')


pf20_df.info()


# Вычислим дневную доходность
pf20train_df = pf20train_df.diff() / pf20train_df.shift(1)#.div(pf20train_df.index.diff().days.to_series().to_list(), axis='index') / pf20train_df.shift(1)
pf20test_df = pf20test_df.diff() / pf20test_df.shift(1)#.div(pf20test_df.index.diff().days.to_series().to_list(), axis='index') / pf20test_df.shift(1)
#pf20train_df = pf20train_df.diff() / (pf20train_df.mul(pf20train_df.index.diff().days.to_series().to_list(), axis='index') - pf20train_df.diff())
#pf20test_df = pf20test_df.diff() / (pf20test_df.mul(pf20test_df.index.diff().days.to_series().to_list(), axis='index') - pf20test_df.diff())


pf20train_df = pf20train_df.iloc[1:]
pf20test_df = pf20test_df.iloc[1:]
pf20train_df.info(), pf20test_df.info()


# Получаем годовые значения
pf20train_covann = pf20train_df.cov()*252
pf20test_covann = pf20test_df.cov()*252
pf20train_covann.max(axis=None)
pf20train_drann = pf20train_df.mean()*252
pf20test_drann = pf20test_df.mean()*252
pf20_drann = pf20train_drann.to_frame()
pf20_drann['class'] = 'train'
pf20_drann = pd.concat([pf20_drann, pf20test_drann]).fillna('test')
pf20_drann.columns = ['annual daily returns', 'class']
plt.figure(figsize=(12, 6))
sns.barplot(pf20_drann, x='Name', y='annual daily returns', hue='class')


fig, ax = plt.subplots(1, 2, figsize=(16, 6))
vmin = 0.975 * min(pf20train_covann.min(axis=None), pf20test_covann.min(axis=None))
vmax = 1.025 * max(pf20train_covann.max(axis=None), pf20test_covann.max(axis=None))
sns.heatmap(pf20train_covann, vmin=vmin, vmax=vmax, ax=ax[0]).set(xlabel=None, ylabel=None, title='Train data')
sns.heatmap(pf20test_covann, vmin=vmin, vmax=vmax, ax=ax[1]).set(xlabel=None, ylabel=None, title='Test data')
fig.suptitle('Annualized covariance matrix for daily returns', fontsize=16, y=0.95)





from qiskit_finance.applications.optimization import PortfolioOptimization
from qiskit_optimization.converters import QuadraticProgramToQubo
from qiskit_algorithms import NumPyMinimumEigensolver
from qiskit_optimization.algorithms import MinimumEigenOptimizer
import pennylane as qml
from pennylane import qaoa
import numpy as np
from scipy.optimize import minimize


def portfolio_optimisation_ex_qaoa(mu, sigma, budget, n_layers, n_iter, n_shots=4096, n_levels=2, simulator="lightning.qubit"):
    n_assets = len(mu)
    risk_factor = 0.5
    n_qubits = n_assets * n_levels
    dev = qml.device(simulator, wires=n_qubits)

    # Classical (exact) solution
    pf = PortfolioOptimization(mu, sigma, risk_factor, budget)
    qp = pf.to_quadratic_program()
    
    converter = QuadraticProgramToQubo()
    qubo = converter.convert(qp)
    penalty_value = converter.penalty
    print(penalty_value)
    
    exact_mes = NumPyMinimumEigensolver()
    optimizer = MinimumEigenOptimizer(exact_mes)
    result_classic = optimizer.solve(qp)
    
    # QAOA solution
    ## Portfolio to Ising problem
    def portfolio_to_ising_coeffs(mu, sigma, budget, penalty_value, risk_factor):
        
        h = np.zeros(n_qubits)
        
        for i in range(n_assets):
            qubit_start = i * n_levels
            # Binary encoding: w_i = sum(2^k * b_{i,k})
            for k in range(n_levels):
                h[qubit_start + k] -= mu[i] * (2**k)
        
        J = np.zeros((n_qubits, n_qubits))
        
        for i in range(n_assets):
            for j in range(i, n_assets):
                if i == j:
                    continue
                else:
                    coeff = sigma[i, j]
                    for k1 in range(n_levels):
                        for k2 in range(n_levels):
                            q1 = i * n_levels + k1
                            q2 = j * n_levels + k2
                            J[q1, q2] += coeff * (2**(k1 + k2))
                            J[q2, q1] += coeff * (2**(k1 + k2))

        J = risk_factor * J

        for i in range(n_assets):
            for j in range(n_assets):
                w_i_scale = sum(2**k for k in range(n_levels))
                w_j_scale = sum(2**k for k in range(n_levels))
                
                for k1 in range(n_levels):
                    for k2 in range(n_levels):
                        q1 = i * n_levels + k1
                        q2 = j * n_levels + k2
                        J[q1, q2] += penalty_value * w_i_scale * w_j_scale
                        J[q2, q1] += penalty_value * w_i_scale * w_j_scale
                
                for k1 in range(n_levels):
                    q1 = i * n_levels + k1
                    h[q1] -= 2 * penalty_value * budget * (2**k1)
        
        return h, J

    # Construct PennyLane Hamiltonian from Ising coeffs
    def ising_to_hamiltonian(h, J):
        coeffs = []
        paulis = []
        
        for i, hi in enumerate(h):
            if abs(hi) > 1e-10:
                coeffs.append(hi)
                paulis.append(qml.PauliZ(i))
        
        for i in range(len(h)):
            for j in range(i+1, len(h)):
                if abs(J[i,j]) > 1e-10:
                    coeffs.append(J[i,j])
                    paulis.append(qml.PauliZ(i) @ qml.PauliZ(j))
        
        return qml.Hamiltonian(coeffs, paulis)
    
    h, J = portfolio_to_ising_coeffs(mu, sigma, budget, penalty_value, risk_factor)

    H_C = ising_to_hamiltonian(h, J)
    H_M = qml.Hamiltonian(np.ones(n_qubits), [qml.PauliX(i) for i in range(n_qubits)])

    ## Energy for minimization
    @qml.qnode(dev)
    def energy(params):
        for i in range(n_qubits): qml.Hadamard(wires=i)
        gammas, betas = params[:n_layers], params[n_layers:]
        for g, b in zip(gammas, betas):
            qml.qaoa.layers.cost_layer(g, hamiltonian=H_C)
            qml.qaoa.layers.mixer_layer(b, hamiltonian=H_M)
        return qml.expval(H_C)

    results_qa_t = []
    def energy_x(*, intermediate_result):
        results_qa_t.append(intermediate_result)
        
    result_qa = minimize(energy, np.random.uniform(0, 2*np.pi, 2*n_layers), method='COBYLA', options={'maxiter': n_iter}, callback=energy_x)

    ## Sampling optimized circuit values
    @qml.qnode(dev, shots=n_shots)
    def sample_portfolio(params):
        for i in range(n_qubits): qml.Hadamard(wires=i)
        gammas, betas = params[:n_layers], params[n_layers:]
        for g, b in zip(gammas, betas):
            qml.qaoa.layers.cost_layer(g, hamiltonian=H_C)
            qml.qaoa.layers.mixer_layer(b, hamiltonian=H_M)
        return qml.sample()

    qa_samples = sample_portfolio(result_qa.x)
    
    ## Expectation values of optimized model
    @qml.qnode(dev)
    def measure_all_z(params):
        for i in range(n_qubits): qml.Hadamard(wires=i)
        gammas, betas = params[:n_layers], params[n_layers:]
        for g, b in zip(gammas, betas):
            qml.qaoa.layers.cost_layer(g, hamiltonian=H_C)
            qml.qaoa.layers.mixer_layer(b, hamiltonian=H_M)
        return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]

    qa_expect = measure_all_z(result_qa.x)

    return result_classic, result_qa, results_qa_t, qa_samples, qa_expect, penalty_value


'''
def decode_discretized(expectation_z):
        weights = []
        for asset in range(n_assets):
            start = asset * n_levels
            bits = np.where(expectation_z[start:start+n_qubits] < 0, 1, 0)
            weight = sum(b * (2**k) for k, b in enumerate(bits)) / (2**n_levels)
            weights.append(weight)
        return np.array(weights)
'''


#mu = pf20train_drann.to_numpy()
#sigma = pf20train_covann.to_numpy()


budget = [3, 4, 6,]
n_layers = [1, 2, 3,]
n_iter = [100, 200, 300,]
n_levels = [1, 2]
n_shots = [1024, 2048, 3072, 4096]


# example
#cc, qa, qa_t, qa_samples, qa_expect, pv = portfolio_optimisation_ex_qaoa(mu, sigma, budget=3, n_layers=2, n_iter=100, n_levels=2)


#cc.x, qa, qa_samples


#qa_samples = pd.DataFrame(qa_samples)
#qa, qa_samples.value_counts().head(), qa, qa_samples.value_counts().value_counts()


#qa_samples.value_counts().reset_index().loc[0].to_numpy()[:10] @ mu, cc.x @ mu


'''
cc_return = cc.x @ mu
qa_bin_return = qa_bin @ mu
qa_return = qa @ mu
cc_risk = np.sqrt(cc.x @ sigma @ cc.x)
qa_bin_risk = np.sqrt(qa_bin @ sigma @ qa_bin)
qa_risk = np.sqrt(qa @ sigma @ qa)
cc_sharpe = (cc_return - 0.045) / cc_risk
qa_bin_sharpe = (qa_bin_return - 0.045) / qa_bin_risk
qa_sharpe = (qa_return - 0.045) / qa_risk
cc_return, qa_bin_return, qa_return, cc_risk, qa_bin_risk, qa_risk, cc_sharpe, qa_bin_sharpe, qa_sharpe, sum(cc.x), sum(qa_bin), sum(qa)
'''


#qa_return = qa @ mu
#qa_risk = np.sqrt(qa @ sigma @ qa)
#qa_sharpe = (qa_return - 0.045) / qa_risk
#cc_return, qa_return, cc_risk, qa_risk, cc_sharpe, qa_sharpe


#from scipy.spatial import distance

#distance.hamming(cc.x, qa_samples.value_counts().reset_index().loc[0].to_numpy()[:10])
#distance.hamming(cc.x, qa_bin)


# получить DF (mu, sigma) X budget X n_layers X n_iter





import os
cds_dc = {f:pd.read_csv(os.path.join('./datasets/credit_scoring', f)) for f in os.listdir('./datasets/credit_scoring')}
cds_dc.keys()
#cds_ls[0].info()


[(k, len(cds_dc[k])) for k in cds_dc.keys()]


cds_dc['Individual.csv'].info()
cds_dc['Individual.csv']['occupation']


cds_dc.keys()


cds_dc['Account_Receivable.csv'].info()
cds_dc['Credit_Rating.csv'].info()
cds_dc['Credit_Rating.csv'].head()


cds_dc['Factoring.csv'].head()
cds_dc['Factoring.csv'].info()


cds_dc['Director.csv'].head()
cds_dc['Director.csv'].info()


cds_dc['Businesses.csv'].head()
cds_dc['Businesses.csv'].info()


cds_dc['Credit_Card_History.csv'].info()


cds_dc.keys()


cds_dc['Account_Receivable.csv'].info()


cds_dc['Account_Receivable.csv']['company_reg_number'].value_counts()


len(set(cds_dc['Businesses.csv']['company_reg_number']) & set(cds_dc['Loan.csv']['company_reg_number']))


cds_dc['Loan.csv'].info()


cds_dc['Loan.csv']['company_reg_number'].value_counts()


#cds_merged = cds_dc['Businesses.csv'].merge(cds_dc['Loan.csv'], on='company_reg_number', how='left', suffixes=('', '_LOAN')).\
#merge(cds_dc['Account_Receivable.csv'], on='company_reg_number', how='left', suffixes=('', '_ACC_RECV')).\
#merge(cds_dc['Credit_Rating.csv'], on='company_reg_number', how='left', suffixes=('', '_CR')).\
#merge(cds_dc['Factoring.csv'], on='company_reg_number', how='left', suffixes=('', '_FACTORING')).\
#merge(cds_dc['Credit_Card_History.csv'], on='company_reg_number', how='left', suffixes=('', '_CREDIT_CARD')).\
#merge(cds_dc['Credit_Account_History.csv'], on='company_reg_number', how='left', suffixes=('', '_CREDIT_ACC'))
#cds_merged.info(max_cols=500)


#cds_merged.dropna(axis=1, how='all', inplace=True)
#cds_merged.info(max_cols=500)
#cds_merged.drop(columns=['Unnamed: 0', 'Unnamed: 0_LOAN',  'Unnamed: 0_ACC_RECV', 'Unnamed: 0_CR',
#                         'Unnamed: 0_FACTORING', 'Unnamed: 0_CREDIT_CARD', 'Unnamed: 0_CREDIT_ACC'], inplace=True)
#cds_merged.info(max_cols=500)


#set(cds_dc['Businesses.csv'].columns) - set(cds_dc['Credit_Account_History.csv'].columns) 


cds_dc['Credit_Account_History.csv'].info()
cds_dc['Credit_Account_History.csv']['company_reg_number'].value_counts()


cds_dc.keys()


#cds_merged = cds_dc['Credit_Account_History.csv'].\
#merge(cds_dc['Loan.csv'], on='company_reg_number', how='left', suffixes=('', '_LOAN')).\
#merge(cds_dc['Account_Receivable.csv'], on='company_reg_number', how='left', suffixes=('', '_ACC_RECV')).\
#merge(cds_dc['Credit_Rating.csv'], on='company_reg_number', how='left', suffixes=('', '_CR')).\
#merge(cds_dc['Factoring.csv'], on='company_reg_number', how='left', suffixes=('', '_FACTORING')).\
#merge(cds_dc['Credit_Card_History.csv'], on='company_reg_number', how='left', suffixes=('', '_CREDIT_CARD')).\
#merge(cds_dc['COVID.csv'], on='company_reg_number', how='left', suffixes=('', '_COVID')).\
#merge(cds_dc['Director.csv'], on='company_reg_n20000015umber', how='left', suffixes=('', '_DIRECTOR'))
#cds_merged.dropna(axis=1, how='all', inplace=True)
#cds_merged.drop(columns=['Unnamed: 0', 'Unnamed: 0_LOAN', 'Unnamed: 0_ACC_RECV', 'Unnamed: 0_CR',
#                         'Unnamed: 0_FACTORING', 'Unnamed: 0_CREDIT_CARD', 'Unnamed: 0_COVID', 'Unnamed: 0_DIRECTOR'], inplace=True)


#cds_merged.info(250)


#cds_merged[['primary_sector', 'primary_sector_LOAN', 'revenue_2019', 'revenue_2019_FACTORING']]
#cds_merged.loc[cds_merged['revenue_2019'] != cds_merged['revenue_2019_FACTORING'], ['revenue_2019', 'revenue_2019_FACTORING']].isna().all()
#cds_merged.drop(columns=['revenue_2019_FACTORING'], inplace=True)
#cds_merged.info()


#cds_merged['primary_sector_LOAN'].equals(cds_merged['primary_sector'])


#cds_merged['revenue_2019_FACTORING'].equals(cds_merged['revenue_2019'])


#cds_merged = cds_merged.drop(columns=['revenue_2019_FACTORING'])


#cds_merged.info(max_cols=250, verbose=True)


#cds_merged.loc[~cds_merged['primary_sector_LOAN'].eq(cds_merged['primary_sector']), ['company_reg_number', 'primary_sector', 'primary_sector_LOAN']]


#cds_merged.loc[~cds_merged['primary_sector_LOAN'].eq(cds_merged['primary_sector']), ['company_reg_number', 'primary_sector', 'primary_sector_LOAN']]['primary_sector_LOAN'].isna().all()


#cds_merged['primary_sector_LOAN'].value_counts()


#cds_dc['Loan.csv'].info()


#cds_merged = cds_merged.drop(columns=['primary_sector_LOAN'])


#cds_dc['Loan.csv']['company_reg_number'].isin(cds_dc['Credit_Account_History.csv']['company_reg_number']).all()


#cds_merged.nunique()
#cds_merged.value_counts()


#cds_dc['COVID.csv'].info(max_cols=150, verbose=True)
#cds_dc['COVID.csv'].nunique()


#cds_merged.info(max_cols=250, verbose=True)


#cds_merged['revenue_2019'].equals(cds_merged['2019_revenue'])


#cds_merged = cds_merged.drop(columns=['2019_revenue'])


#cds_merged[['current_account_number', 'current_account_numbers']].value_counts()


#cds_merged[['current_account_numbers_list', 'current_account_numbers']].value_counts()


#cds_merged[['current_account_numbers_list', 'current_account_number']].value_counts()


#cds_merged = cds_merged.drop(columns=['current_account_numbers', 'current_account_numbers_list'])


#cds_merged['ca_start_date'].value_counts()


#cds_merged['turnover_bands'].value_counts()


#cds_merged.info(max_cols=250, verbose=True)


cds_dc['Credit_Account_History.csv'].info()


cds_dc['Loan.csv'].info(), cds_dc['Credit_Card_History.csv'].info()


cds_dc['Loan.csv']['company_reg_number'].nunique(), cds_dc['Credit_Card_History.csv']['company_reg_number'].nunique()


cds_dc['Loan.csv']['company_reg_number'].isin(cds_dc['Credit_Card_History.csv']['company_reg_number']).value_counts()


cds_dc['Credit_Rating.csv'].info()
cds_dc['Credit_Rating.csv'].head()


cds_dc.keys()


#cds_dc['Businesses.csv'].info()
#cds_dc['Credit_Account_History.csv'].info()
set(cds_dc['Businesses.csv'].columns) - set(cds_dc['Credit_Account_History.csv'].columns)


#cds_dc['COVID.csv'].info(max_cols=250, verbose=True)


#cds_merged = cds_dc['Loan.csv'].\
#merge(cds_dc['Credit_Card_History.csv'], on='company_reg_number', how='outer', suffixes=('_LOAN', '_CC')).\
#merge(cds_dc['Credit_Account_History.csv'], on='company_reg_number', how='left', suffixes=('', '_CA')).\
#merge(cds_dc['Director.csv'], on='company_reg_number', how='left', suffixes=('', '_DIRECTOR')).\
#merge(cds_dc['Factoring.csv'], on='company_reg_number', how='left', suffixes=('', '_FACTORING')).\
#merge(cds_dc['Credit_Rating.csv'], on='company_reg_number', how='left', suffixes=('', '_CR')).\
#merge(cds_dc['Account_Receivable.csv'], on='company_reg_number', how='left', suffixes=('', '_RECIAVABLE'))
#merge(cds_dc['COVID.csv'], on='company_reg_number', how='left', suffixes=('', '_COVID'))
#cds_merged.drop(columns=['Unnamed: 0_LOAN', 'Unnamed: 0_CC'], inplace=True)
#cds_merged.info(max_cols=250, verbose=True)


ca_df = cds_dc['Credit_Account_History.csv'].copy()
ca_df.drop(columns=['Unnamed: 0', '2019_revenue', 'current_account_numbers', 'current_account_numbers_list',
                    'date_of_name_change', 'previous_name'], inplace=True)
#ca_df.drop_duplicates()
ca_df['company_reg_number'].value_counts()
ca_df[ca_df['company_reg_number'] == 20000015]


# Изучаем кредитную историю
loans_df = cds_dc['Loan.csv'].copy()
loans_df = loans_df.convert_dtypes()
loans_df.info()


loans_df['loan_id'].unique().all(), loans_df['interest'].median()


loans_df = loans_df.drop(columns=['loan_id', 'Unnamed: 0'])
loans_df = loans_df.drop(loans_df[loans_df['loan_status'].isin(['Disbursing', 'Available'])].index)
loans_df = loans_df.fillna(value={'early_repayment_allowed': 1})
loans_df.info()


loans_df['company_reg_number'].value_counts()
loans_df[loans_df['company_reg_number'] == 20000984]


loans_df = loans_df.drop(88)
cds_dc['Loan.csv']['loan_amount_outstanding_including_future_interest'].value_counts()
loans_df['interest'].value_counts()
loans_df = loans_df.drop(columns=['loan_default_date', 'loan_satisfaction_date', 'loan_amount_outstanding_including_future_interest', 'interest'])


loans_df.info()


loans_df.describe()


# Выделяем кредитные данные, критичные для разделения по классам
loans_cs_df = loans_df[['company_reg_number', 'loan_status', 'loan_number_of_missed_payments']]
loans_cs_df.info()


# Изучаем историю по кредитным картам
cc_df = cds_dc['Credit_Card_History.csv'].copy()
cc_df.head(), cc_df.info()


cc_df[~cc_df['cc_missed_payments'].fillna(0).eq(cc_df['missed_payments_number'])]


cc_df['missed_payments_number'].value_counts().sort_index()
#cc_df['cc_missed_payments'].value_counts()


min(cc_df['cc_start_date']), max(cc_df['cc_start_date'])


cc_df = cc_df.convert_dtypes()
cc_df['cc_start_date'] = pd.to_datetime(cc_df['cc_start_date'])
cc_df = cc_df[cc_df['cc_start_date'] > (max(cc_df['cc_start_date']) - pd.DateOffset(years=5))]
cc_df.info()


cc_df['company_reg_number'].value_counts()
cc_df['missed_payments_number_total'] = cc_df.groupby('company_reg_number')['missed_payments_number'].transform('sum')
cc_df.head()


# Выделяем данные по кредитным картам, критичные для разделения по классам
cc_cs_df = cc_df[['company_reg_number', 'missed_payments_number_total']].drop_duplicates()
cc_cs_df.info()


# Объединяем данные по кредитам и по кредитным картам, критичные для разделения по классам
cs_df = loans_cs_df.merge(cc_cs_df, on='company_reg_number', how='outer')


cs_df['loan_number_of_missed_payments'].value_counts()


# Формируем классы для сэмплов
cs_df['potential_defaulter'] = cs_df['loan_number_of_missed_payments'] > 0
cs_df['defaulter'] = cs_df['loan_status'] == 'Defaulted'
cs_df['cc_potential_defaulter'] = cs_df['missed_payments_number_total'] > 0
cs_df['cc_defaulter'] = cs_df['missed_payments_number_total'] >= 5
cs_df.drop(columns=['loan_status', 'loan_number_of_missed_payments', 'missed_payments_number_total'], inplace=True)
cs_df.fillna(False, inplace=True)
cs_df.info()
cs_df[cs_df.columns[1:]].apply(pd.Series.value_counts)


cds_dc.keys()


#cs_df.merge(cds_dc['Businesses.csv'], on='company_reg_number', how='left')
#cds_dc['Account_Receivable.csv'].value_counts()


#cds_dc['Director.csv'][cds_dc['Director.csv']['termination'] == 'Present'].describe()#['indv_id'].value_counts()
#cds_dc['Director.csv'][cds_dc['Director.csv']['termination'] == 'Present']
cds_dc['Director.csv'].describe()
#cds_dc['Director.csv']['indv_id'].nunique()
#cds_dc['Director.csv']['termination'].value_counts()


dr_df = cds_dc['Director.csv'].drop(columns=['employment_status', 'disqual', 'start_date_of_disqual', 'end_date_of_disqual', 'active_company'])
dr_df.info()


dr_df[['indv_id', 'officer', 'ubo']].value_counts().sort_index()
dr_df.loc[dr_df['termination'] == 'Present', ['company_reg_number', 'officer', 'ubo']].value_counts().sort_index().value_counts()


# Объединяем данные по классам с характеристиками классов
cs2_df = cs_df.\
merge(cds_dc['Credit_Rating.csv'], on='company_reg_number', how='left', suffixes=('', '_CR')).\
merge(cds_dc['Factoring.csv'], on='company_reg_number', how='left', suffixes=('', '_FACTORING')).\
merge(dr_df, on='company_reg_number', how='left', suffixes=('', '_DIRECTOR')).\
merge(cds_dc['COVID.csv'], on='company_reg_number', how='left', suffixes=('', '_COVID')).\
merge(cds_dc['Credit_Account_History.csv'], on='company_reg_number', how='left', suffixes=('', '_CA'))
cs2_df.info(max_cols=250, verbose=True)


cs2_df.drop(columns=['Unnamed: 0', 'Unnamed: 0_FACTORING', 'Unnamed: 0_DIRECTOR', 'Unnamed: 0_COVID', 'Unnamed: 0_CA',
                    '2019_revenue', 'revenue_2019_CA', 'date_of_name_change', 'previous_name'], inplace=True)
cs2_df.info(max_cols=250, verbose=True)


cs2_df.loc[cs2_df['credit_report_agency'].isna(), cs2_df.columns[5:20]].isna().all()


cs2_df = cs2_df.dropna(subset='credit_report_agency')
cs2_df.info(max_cols=250, verbose=True)


cs2_df[['revenue_2019', 'factor_amount', 'factor_percent']].describe()
cs2_df[['factor_percent']].value_counts()


cs2_df['factoring_type'].value_counts()


cs2_df.fillna(value={'revenue_2019': 0.0, 'factor_amount': 0.0, 'factor_percent': 0.0}, inplace=True)
cs2_df.info(max_cols=250, verbose=True)


cs2_df.loc[:, cs2_df.columns[115:132]].describe()


cs2_df.loc[:, cs2_df.columns[115:132]] = \
cs2_df.loc[:, cs2_df.columns[115:132]].fillna(cs2_df.loc[:, cs2_df.columns[115:132]].describe().loc['50%',:])
cs2_df.loc[:, cs2_df.columns[115:132]].info()


cs2_df[cs2_df.columns[166:179]].describe()


cs2_df.loc[cs2_df['capex'].isna(), cs2_df.columns[166:179]].isna().all()


cs2_df.dropna(subset='capex', inplace=True)
cs2_df.info(max_cols=250, verbose=True)


# Переходим к feature selection
cs3_df = cs2_df.convert_dtypes()


cs3_df.info(max_cols=250, verbose=True)


cs3_df[cs3_df.columns[5:8]].value_counts()


cs3_df[cs3_df.columns[7]].value_counts(), cs3_df[cs3_df.columns[6]].value_counts()


import datetime as dt
cs3_df[cs3_df.columns[6]] = pd.to_datetime(cs3_df[cs3_df.columns[6]]).apply(lambda x: dt.datetime.toordinal(x))
# 5, 7 to ohc, na-ignore (one-hot-encoder)


cs3_df[cs3_df.columns[23:25]].apply(pd.Series.value_counts)
# 23, 24 to ohc, na-ignore


cs3_df[cs3_df.columns[29:45]].head()


cs3_df[cs3_df.columns[29:33]].apply(pd.Series.value_counts)


cs3_df[cs3_df.columns[30]] = cs3_df[cs3_df.columns[30]].replace('Present', pd.NA)
for i in range(29, 33):
    cs3_df[cs3_df.columns[i]] = pd.to_datetime(cs3_df[cs3_df.columns[i]]).apply(lambda x: dt.datetime.toordinal(x))
cs3_df.loc[cs3_df[cs3_df.columns[30]] == 1, cs3_df.columns[30]] = max(cs3_df[cs3_df.columns[30]]) + 365
cs3_df[cs3_df.columns[29:33]].describe()


cs3_df[cs3_df.columns[33:45]].apply(pd.Series.value_counts)
# 33:45 to orc (ordinal encoder)


cs3_df[cs3_df.columns[57:115]].head()


#cs3_df[cs3_df.columns[57]].value_counts()
cs3_df[cs3_df.columns[58:115]].apply(pd.Series.value_counts)
cs3_df[cs3_df.columns[58:115]] = cs3_df[cs3_df.columns[58:115]].replace(['Not sure', 'Not applicable'], pd.NA)
# 57 to ohc, na-ignore
# 58:115 to orc


cs3_df[cs3_df.columns[132:147]].apply(pd.Series.value_counts)
cs3_df[cs3_df.columns[132:147]] = cs3_df[cs3_df.columns[132:147]].replace(['Not sure', 'Not applicable'], pd.NA)
# 132:147 to orc


cs3_df[cs3_df.columns[148:155]].apply(pd.Series.value_counts)
# 148:155 to ohc (+ preprocessing i.e. BoW ?)
# 155 to orc


cs3_df[cs3_df.columns[158]] = pd.to_datetime(cs3_df[cs3_df.columns[158]]).apply(lambda x: dt.datetime.toordinal(x))


cs3_df[cs3_df.columns[160:165]].apply(pd.Series.value_counts)


cs3_df[cs3_df.columns[160]] = pd.to_datetime(cs3_df[cs3_df.columns[160]]).apply(lambda x: dt.datetime.toordinal(x))
cs3_df.drop(columns=['country_of_incorporation', 'country_of_primary_operation'], inplace=True)
cs3_df.info(max_cols=250, verbose=True)
# 161, 162 - ? check crossing data with Director.csv !


cs3_df[cs3_df.columns[168]].value_counts()
# 168 - to orc


cs3_df[cs3_df.columns[173]].value_counts()


cs3_df[cs3_df.columns[173]] = pd.to_datetime(cs3_df[cs3_df.columns[173]]).apply(lambda x: dt.datetime.toordinal(x))


cs3_df[cs3_df.columns[180:183]].apply(pd.Series.value_counts)
#cs3_df[cs3_df.columns[132:147]] = cs3_df[cs3_df.columns[132:147]].replace(['Not sure', 'Not applicable'], pd.NA)
cs3_df[cs3_df.columns[182]] = pd.to_datetime(cs3_df[cs3_df.columns[182]]).apply(lambda x: dt.datetime.toordinal(x))
# 180 - extract to several numeric features
# 181 - convert to numeric features (MB PRE-MERGE!)


# Данные с подготовленным планом для энкодинга + несколько возможных переработок
# 5, 7, 23, 24, 57, 148:155 - to ohc, na-ignore
# 33:45, 58:115, 132:147, 155, 168  - to orc
# 148:155 (some of), 161, 162, 180, 181 - transform?
# combine prices_waves, effect_waves?
cs4_df = cs3_df.reset_index()
cs4_df.info(max_cols=200, verbose=True)


cs4_df[cs4_df.columns[181]].value_counts(), cs4_df[cs4_df.columns[182]].value_counts()


cs4_df.sort_values(by='company_reg_number', ascending=False)[cs4_df.columns[178:186]].sort_values(by='number_of_accounts', ascending=False).head(10)


cs4_df['number_of_accounts'].value_counts()


cs4_df.groupby('company_reg_number')['current_acc_fraction'].apply(lambda x: (x.iloc[0] == x).all()).all()
cs4_df.groupby('company_reg_number')['total_amount'].apply(lambda x: (x.iloc[0] == x).all()).all()


cs4_df = cs4_df.drop(columns=['current_account_numbers',	'current_account_numbers_list'])
cs4_df.sort_values(by='company_reg_number', ascending=False)[cs4_df.columns[178:184]].sort_values(by='number_of_accounts', ascending=False).head(10)


cs4_df.groupby('company_reg_number')['ca_start_date'].agg('count')#rank(method='first')


cs4_df.loc[cs4_df['company_reg_number'] == 20000977, cs4_df.columns[178:184]]


max(cs4_df.groupby('company_reg_number')['current_account_number'].agg('nunique'))#rank(method='first')


cds_dc.keys()
csac_df = cds_dc['Credit_Account_History.csv']


csac_df.columns
csac_df = csac_df.drop(columns=['Unnamed: 0', '2019_revenue', 'country_of_incorporation', 'country_of_primary_operation',
                                'current_account_numbers', 'current_account_numbers_list', 'date_of_name_change', 'previous_name'])


csac_df['ca_start_date'] = pd.to_datetime(csac_df['ca_start_date']).apply(lambda x: dt.datetime.toordinal(x))
csac_df['filing_date'] = pd.to_datetime(csac_df['filing_date']).apply(lambda x: dt.datetime.toordinal(x))
csac_df.info()


csac_df.sort_values(by=['number_of_accounts', 'company_reg_number'], ascending=False)[csac_df.columns[22:]].head(10)
#csac_df[csac_df.columns[25]].value_counts().size
#csac_df[csac_df.columns[22:31]].apply(pd.Series.value_counts)


csac_df[['current_account_number', 'rev_ratio']].value_counts().value_counts()#sort_values()
#csac_df['current_account_number'].value_counts()


# TODO: pivot 'ca_start_date', 'current_account_number', 'amount', 'pay_in_amount', 'pay_out_amount'


csac_df['timerank'] = csac_df.sort_values('ca_start_date', ascending=False).groupby('company_reg_number')['ca_start_date'].rank(method='first').astype(int)#transform(lambda x: x.sort_values())


csac_df.sort_values(by=['number_of_accounts', 'company_reg_number'], ascending=False)[csac_df.columns[22:]].head(10)


tst = csac_df.pivot(index='company_reg_number', columns='timerank', values=['ca_start_date', 'current_account_number', 'amount', 'pay_in_amount', 'pay_out_amount'])


#tst.columns, tst.columns.set_levels([f'{a}_{b}' for a, b in tst.columns.to_list()], level=1)#.droplevel(0)#apply(lambda x: x.join())
#[(a, f'{a}_{b}') for a, b in tst.columns.to_list()]
tst.columns, pd.MultiIndex.from_tuples([(a, f'{a}_{b}') for a, b in tst.columns.to_list()]).droplevel(0)
tst2 = tst.copy()
tst2.columns = pd.MultiIndex.from_tuples([(a, f'{a}_{b}') for a, b in tst.columns.to_list()]).droplevel(0)
tst2.reset_index()


cs5_df = cs4_df.drop(columns=['ca_start_date', 'current_account_number', 'amount', 'pay_in_amount', 'pay_out_amount']).merge(tst2, on='company_reg_number', how='left').drop(columns='index').drop_duplicates()


cs5_df.shape


# Заполняем NA значения в Account Data
values = {f'{i}_{j}': 0.0 for i in ['amount', 'pay_in_amount', 'pay_out_amount'] for j in range(1, 5)}
cs5_df.fillna(values, inplace=True)


cs5_df[['ca_start_date_2', 'ca_start_date_3', 'ca_start_date_4']] = \
cs5_df[['ca_start_date_2', 'ca_start_date_3', 'ca_start_date_4', 'ca_start_date_1']].bfill(axis=1)[['ca_start_date_2', 'ca_start_date_3', 'ca_start_date_4']]   


tmp_acc_df = cs5_df[['current_account_number_2', 'current_account_number_3', 'current_account_number_4', 'current_account_number_1']]
tmp_acc_df['current_account_number_1'] = - tmp_acc_df['current_account_number_1']
cs5_df[['current_account_number_2', 'current_account_number_3', 'current_account_number_4']] = \
tmp_acc_df.bfill(axis=1)[['current_account_number_2', 'current_account_number_3', 'current_account_number_4']]


# Исследуем Director данные
cds_dc.keys()
cds_dc['Director.csv'].info()


dic_df = cs5_df[cs5_df.columns[25:33]]
dic_df.head(10)


(dic_df['officer'] | dic_df['ubo'] == dic_df['current_employee_at_company']).all()


#cs5_df.drop(columns=['current_employee_at_company'], inplace=True)


cs5_df.groupby(['company_reg_number', 'officer'])['ubo'].apply(lambda x: x.nunique())


cs5_df.loc[cs5_df['company_reg_number'] == 20000999, ['officer', 'ubo']]


#cs5_df.columns[-50:]
cs5_df.sort_values('company_reg_number')[['company_reg_number', 'indv_id', 'officer', 'ubo', 'officers_and_percent_owned', 'ubo_and_percent_owned']].head(10)


cs5_df['indv_id'].nunique()


max(cs5_df.groupby(['company_reg_number', 'officer'])['ubo'].apply(lambda x: x.nunique()))
cs5_df.groupby(['company_reg_number', 'officer'])['ubo'].apply(lambda x: x.nunique()).value_counts()


cs4_df.loc[cs4_df['current_employee_at_company'] != 0, ['indv_id', 'officer', 'ubo']].value_counts().reset_index()
cs4_df.loc[cs4_df['current_employee_at_company'] != 0, ['indv_id', 'officer', 'ubo']].value_counts().reset_index()['indv_id'].nunique()
#cs4_df.loc[~cs4_df['indv_id'].isin(nonu_id) & cs4_df['current_employee_at_company'] != 0, ['indv_id', 'officer', 'ubo']].sort_values(['indv_id', 'officer'])
#nonu_id
#cs4_df[cs4_df['indv_id']]


#cs5_df[['company_reg_number', 'incorp_date']].nunique()#value_counts()
#cs5_df['company_reg_number'].value_counts()
max(cs5_df.groupby('company_reg_number')['incorp_date'].apply(lambda x: x.nunique()))


cs5_df.reset_index(inplace=True, drop=True)


# Extracting percent_owned values from 'officers_and_percent_owned' and 'ubo_and_percent_owned' columns
import ast
#percent_tmp = cs5_df['officers_and_percent_owned'] + cs5_df['ubo_and_percent_owned']
percent_tmp = \
cs5_df['officers_and_percent_owned'].apply(lambda x: ast.literal_eval(x)) + cs5_df['ubo_and_percent_owned'].apply(lambda x: ast.literal_eval(x))
percent_tmp


percent_dict = percent_tmp.apply(lambda x: dict(x))
percent_dict


perc_tmp = pd.concat([cs5_df[['indv_id', 'officer', 'ubo']], percent_dict], axis=1)
perc_tmp.loc[(perc_tmp['officer'] == 1) & (perc_tmp['ubo'] == 0)].apply(lambda x: x[0].get(x['indv_id'], pd.NA), axis=1).nunique()


cs5_df.loc[cs5_df['ubo'] == 1, ['indv_id']].nunique()
cs5_df['company_reg_number'].nunique()


max(cs5_df.groupby('company_reg_number')['ubo'].agg('sum')), max(cs5_df[cs5_df['ubo'] != 0].groupby(['indv_id', 'ubo'])['company_reg_number'].agg('count'))


#cs5_df['ubo'] = perc_tmp.apply(lambda x: x[0].get(x['indv_id'], 0), axis=1)


cs5_df.drop(columns=['officers_and_percent_owned', 'ubo_and_percent_owned'], inplace=True)


cs5_df.pivot(index='company_reg_number', columns='indv_id', values=['officer', 'ubo', 'current_employee_at_company'])#, 'appointment'])#, 'termination'])


cs5_df.pivot(index='company_reg_number', columns='indv_id', values=['officer', 'ubo'])#, 'termination'])



# Encoding individual roles
role = 1 * cs5_df['current_employee_at_company'].replace([1, 0], [0, -1]) + \
1 * cs5_df['officer'] + \
2 * cs5_df['ubo']
role


cs5_df['role'] = role
cs5_df['percent_owned'] = perc_tmp.apply(lambda x: x[0].get(x['indv_id'], 0), axis=1)


cs5_df[['company_reg_number', 'indv_id', 'role', 'percent_owned', 'appointment', 'termination']].\
pivot(index='company_reg_number', columns='indv_id', values=['role', 'percent_owned'])#, 'appointment', 'termination'])#, 'termination'])
#cs5_df


# Reducing appointment and termination data to latest values per company
#cs5_df['last_appointment'] = 
cs5_df.groupby('company_reg_number')['appointment'].transform(max)
cs5_df.groupby('company_reg_number')['termination'].transform(max).nunique()
cs5_df['latest_appointment'] = cs5_df.groupby('company_reg_number')['appointment'].transform(max)


max(cs5_df.groupby('company_reg_number')['termination'].transform(max))
cs5_df['latest_termination'] = cs5_df.replace({'termination': 770663}, 0).groupby('company_reg_number')['termination'].transform(max)


cs4_df['termination'].value_counts().value_counts()
cs5_df['termination'].value_counts()


pre_pivot = cs5_df[['company_reg_number', 'indv_id', 'role', 'percent_owned']].\
pivot(index='company_reg_number', columns='indv_id', values=['role', 'percent_owned'])


pre_pivot.columns = pd.MultiIndex.from_tuples([(a, f'{a}_{b}') for a, b in pre_pivot.columns.to_list()]).droplevel(0)


# Pivoting Director data
cs6_df = cs5_df.merge(pre_pivot.reset_index().fillna(0), on='company_reg_number', how='left')
cs6_df = cs6_df.drop(columns=['indv_id', 'officer', 'ubo', 'current_employee_at_company', 'appointment', 'termination', 'dob', 'role', 'percent_owned']).drop_duplicates()
cs6_df.info(max_cols=1000, verbose=True)


# Filling NA data for Latest Termination feature
cs6_df[['latest_termination', 'latest_appointment']].max()
cs6_df.replace({'latest_termination': 0}, cs6_df['latest_termination'].max() + 365, inplace=True)


cs6_df.info(max_cols=1000, verbose=True)


cs6_df[cs6_df.columns[148:150]].head()


cs6_df[cs6_df.columns[6]].value_counts()


cs6_df[cs6_df.columns[7]].value_counts()


cs6_df[cs6_df.columns[8:10]].head()


cs6_df[cs6_df.columns[14:23]].head()


cs6_df[cs6_df.columns[23:25]].head()
cs6_df[cs6_df.columns[23:25]].value_counts()


cs6_df[cs6_df.columns[26:38]].head()


cs6_df[cs6_df.columns[38:50]].apply(pd.Series.value_counts)


cs6_df[cs6_df.columns[50]].value_counts()


cs6_df[cs6_df.columns[51:54]].head()


cs6_df[cs6_df.columns[108:125]].head()


cs6_df[cs6_df.columns[125:140]].head()


cs6_df[cs6_df.columns[140]].value_counts()


cs6_df[cs6_df.columns[141:150]].head()


cs6_df[cs6_df.columns[150:155]].head()
cs6_df[cs6_df.columns[152]].value_counts()#.unique()
cs6_df[cs6_df.columns[154]].value_counts()#.unique()


cs6_df[cs6_df.columns[155:160]].head()


cs6_df[cs6_df.columns[160:169]].head()


cs6_df[cs6_df.columns[169:173]].head()


# RE: Данные с подготовленным планом для энкодинга + несколько возможных переработок
# 1:5 - labels
# 5, 7, 24, 50, 141:148 - to ohc, na-ignore
# 23, 26:38, 51:108, 125:140, 148, 149, 159,   - to orc
# 6, 8:23, 25, 108:125, 140, 150:152, 153, 155:159, 160:169, 169:173, 173:177, 181: - orc ready
# 38:50, 152, 154, 177:181  - ohc ready
# 142, 145, 147 - transform? , 145, 147 - compare, 152 - drop?


# Exploring entity_name data
cs6_df['entity_trade_name'].eq(cs6_df['entity_name'])
cs6_df.loc[0, ['entity_trade_name', 'entity_name']]
print(cs6_df.loc[0, 'entity_trade_name'])
print(cs6_df.loc[0, 'entity_name'])
cs6_df['entity_trade_name'].nunique(), cs6_df['entity_name'].nunique()


from sklearn.feature_extraction.text import CountVectorizer

vec_etn = CountVectorizer()
etname_v = vec_etn.fit_transform(cs6_df['entity_trade_name'])
vec_etn.get_feature_names_out()
pd.DataFrame(etname_v.toarray()).apply(lambda x: x.where(x != 0).count()).sort_values()#pd.Series.value_counts)


# Exploring address data
vec_add = CountVectorizer()
address_v = vec_add.fit_transform(cs6_df['address'])
vec_add.get_feature_names_out()
pd.DataFrame(address_v.toarray()).apply(lambda x: x.where(x != 0).count()).sort_values()#pd.Series.value_counts)


cs6_df['address'].sample(10)
cs6_df['address'].nunique()


# Extracting postal code
cs6_df['address'].apply(lambda x: x.split()[-1]).nunique()


cs6_df['entity_trade_name'].nunique()


# Transforming entity_name data
etn_len_df = cs6_df['entity_trade_name'].apply(lambda x: len(x.split(',')))
etn_len_df
en_diff_df = cs6_df[['entity_trade_name', 'entity_name']].\
apply(lambda x: len(set(x['entity_trade_name'].split(', ')) ^ set((e.split(' ')[0] for e in x['entity_name'].split(', ')))), axis=1)
en_diff_df.value_counts()
#cs6_df['entity_name'].apply(lambda x: x.split()[-1]).value_counts()
#cs6_df['entity_name'].apply(lambda x: x.split(', '))[0]


"as asd".removesuffix(r'asd')
'a'.split(' ')
set(range(1, 4))


cs6_df.rename(columns={'entity_trade_name': 'etn_length', 'entity_name': 'etn_len_sdiff', 'address': 'postal_code'}, inplace=True)
cs6_df['etn_length'] = etn_len_df
cs6_df['etn_len_sdiff'] = en_diff_df
cs6_df['postal_code'] = cs6_df['postal_code'].apply(lambda x: x.split()[-1])
cs7_df = cs6_df.drop(columns=['entity_status']).reset_index(drop=True)


#cs7_df.info(max_cols=1000, verbose=True)


cs7_df[cs7_df.columns[148:150]].sample(3)


cs7_df[cs7_df.columns[150:154]].sample(3)


# RE:RE: Данные с подготовленным планом для энкодинга
# 1:5 - labels
# 7, 24, 50, 141:145, 146 - to ohc, na-ignore
# 23, 26:38, 51:108, 125:140, 148, 149, 158   - to orc
# 6, 8:23, 25, 108:125, 140, 145, 147, 150:153, 154:158, 159:168, 168:172, 172:176, 180: - orc ready
# 5, 38:50, 153, 176:180  - ohc ready


# --- categorical encoding ---
cs7_df[['credit_report_agency', 'credit_report_model', 'factoring_provider', 'online_challenge_wave13', 'company_type']].sample(5)
cs7_df[['credit_report_agency', 'credit_report_model', 'factoring_provider', 'online_challenge_wave13', 'company_type']].info()


cs7_df[['credit_report_agency', 'credit_report_model', 'factoring_provider', 'online_challenge_wave13', 'company_type']].apply(pd.Series.value_counts)


from sklearn.preprocessing import OneHotEncoder

ohc1 = OneHotEncoder(sparse_output=False).set_output(transform='pandas')
ohc_df0 = cs7_df[['credit_report_agency', 'credit_report_model', 'company_type', 'borough_county',
                  'primary_sector', 'factoring_provider', 'online_challenge_wave13']]
ohc_df0.fillna({'factoring_provider': 'NA', 'online_challenge_wave13': 'NA'}, inplace=True)
ohc_df = ohc1.fit_transform(ohc_df0).drop(columns=['factoring_provider_NA', 'online_challenge_wave13_NA'])
ohc_df.sample(3)


cs7_df[cs7_df.columns[141:145]].sample(5)
cs7_df[cs7_df.columns[141:145]].info()
cs7_df[cs7_df.columns[144]].value_counts()
cs7_df[cs7_df.columns[143]].nunique()
cs7_df[cs7_df.columns[142]].nunique()
# TODO: delete postal_code, contact_phone_number columns


cs7_df[cs7_df.columns[38:50]].sample(5)


ohc1.fit_transform(cs7_df[cs7_df.columns[176:180]])


csac_df.groupby(['current_account_number'])['company_reg_number'].agg('count').max()


# TODO: delete postal_code, contact_phone_number, current_account_number columns
# --- ordinal encoding ---
# 23, 26:38, 51:108, 125:140, 148, 149, 158   - to orc
# 6, 8:23, 25, 108:125, 140, 145, 147, 150:153, 154:158, 159:168, 168:172, 172:176, 180: - orc ready


cs7_df.info(max_cols=1000, verbose=True)


cs7_df[cs7_df.columns[23]].value_counts()


cs7_df[cs7_df.columns[26:38]].info()
cs7_df[cs7_df.columns[26:38]].apply(pd.Series.value_counts)


[[1, 2]] + 2 *[[1, 2]]


cs7_df[cs7_df.columns[51:54]].apply(pd.Series.value_counts)


cs7_df[cs7_df.columns[54:78]].apply(pd.Series.value_counts)


cs7_df[cs7_df.columns[78:89]].apply(pd.Series.value_counts)


cs7_df[cs7_df.columns[89:100]].apply(pd.Series.value_counts)


cs7_df[cs7_df.columns[100:108]].apply(pd.Series.value_counts)


cs7_df[cs7_df.columns[125:129]].apply(pd.Series.value_counts)


cs7_df[cs7_df.columns[129:140]].apply(pd.Series.value_counts)


cs7_df[cs7_df.columns[148:150]].apply(pd.Series.value_counts)


cs7_df[cs7_df.columns[158]].value_counts()


import numpy as np
from sklearn.preprocessing import OrdinalEncoder

categories = \
[['Whole_turnover_recourse', 'Selective_recourse', 'Whole_turnover_non_recourse']] + \
12*[['Has permanently ceased trading **', 'Has temporarily closed or temporarily paused trading', 'Currently trading']] + \
3*[['Footfall has decreased', 'Footfall has stayed the same', 'Footfall has increased']] + \
24*[['Prices decreased more than normal', 'Some prices increased, some prices decreased', 'Prices did not change any more than normal', 'Prices increased more than normal']] + \
11*[['Not been able to export in the last two weeks', 'Exporting, but less than normal', 'Exporting has not been affected', 'Exporting more than normal']] + \
11*[['Not been able to import in the last two weeks', 'Importing, but less than normal', 'Importing has not been affected', 'Importing more than normal']] + \
8*[['Stock levels are lower than normal', 'Stock levels have not changed', 'Stock levels are higher than normal']] + \
4*[['Capital expenditure has stopped', 'Capital expenditure is lower than normal', 'Capital expenditure has not been affected', 'Capital expenditure is higher than normal']] + \
11*[['No cash reserves', 'Less than 1 month', '1 to 3 months', '4 to 6 months', 'More than 6 months']] + \
[['0-632k', '632k-10.2M']] + \
[['0-4 People', '5-9 People', '10-19 People', '20-49 People']] + \
[['0-632k', '632k-1M', '1M-10.2M']]
columns_list = \
cs7_df.columns[23:24].to_list() + cs7_df.columns[26:38].to_list() + cs7_df.columns[51:54].to_list() + \
cs7_df.columns[54:78].to_list() + cs7_df.columns[78:89].to_list() + cs7_df.columns[89:100].to_list() + \
cs7_df.columns[100:108].to_list() + cs7_df.columns[125:129].to_list() + cs7_df.columns[129:140].to_list() + \
cs7_df.columns[148:150].to_list() + cs7_df.columns[158:159].to_list()
orc = OrdinalEncoder(categories=categories, handle_unknown='use_encoded_value', unknown_value=-1).set_output(transform='pandas')
orc_df = orc.fit_transform(cs7_df[columns_list])
#orc_df
#cs7_df.loc[0, ['factoring_type', 'trading_status_wave03']]


# --- bringing categorical and ordinal encoding together ---
cs8_df = cs7_df.copy()
cs8_df[columns_list] = orc_df
cs8_df.drop(columns=['credit_report_agency', 'credit_report_model', 'company_type', 'borough_county', 'primary_sector',
                     'factoring_provider', 'online_challenge_wave13', 'postal_code', 'contact_phone_no',
                     'current_account_number_1', 'current_account_number_2', 'current_account_number_3', 'current_account_number_4',
                     'company_reg_number'], inplace=True)
cs8_df = pd.concat([cs8_df, ohc_df], axis=1)
cs8_df = cs8_df.convert_dtypes()
cs8_df.info(max_cols=1000, verbose=True)


#sns.heatmap(cs8_df.corr())


cs8_df[['cc_potential_defaulter', 'potential_defaulter', 'cc_defaulter', 'defaulter']].describe()


# lkg data split, scale
from sklearn.preprocessing import label_binarize
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


y = label_binarize(cs8_df[['cc_potential_defaulter', 'potential_defaulter', 'cc_defaulter', 'defaulter']],
                   classes=['cc_potential_defaulter', 'potential_defaulter', 'cc_defaulter', 'defaulter'])
y_coded = y# @ np.array([1, 2, 4, 8])
# набор данных очень небольшой и группы несбалансированы,
# попробуем сократить набор классов
y2 = pd.concat([cs8_df['cc_potential_defaulter'] | cs8_df['potential_defaulter'], (cs8_df['cc_defaulter'] | cs8_df['defaulter'])], axis=1)
y2_coded = y2 @ np.array([1, 2])
y_coded = y2_coded.astype(int)
y3_coded = cs8_df[['cc_potential_defaulter', 'potential_defaulter', 'cc_defaulter', 'defaulter']].any(axis=1).to_numpy().astype(int)
#y_coded = y3_coded
X = cs8_df[cs8_df.columns[5:]].to_numpy()
X_train, X_test, y_train, y_test = train_test_split(X, y_coded, random_state=30, test_size=.2)

scaler = StandardScaler()
scaler.fit(X_train)
Xsc_train = scaler.transform(X_train)
Xsc_test = scaler.transform(X_test)


pd.DataFrame(y_train).value_counts(), pd.DataFrame(y_train).value_counts(normalize=True), pd.DataFrame(y_test).value_counts()
#use SMOTE?


from imblearn.over_sampling import SVMSMOTE, SMOTE, ADASYN


smote = SVMSMOTE(random_state=33)
X_res, y_res = smote.fit_resample(Xsc_train, y_train)
pd.DataFrame(y_res).value_counts(normalize=True)


# PCA or LDA, PCA first
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis


lda = LinearDiscriminantAnalysis()
lda.fit(X_res, y_res)

Xlda_train = lda.transform(X_res)
Xlda_test = lda.transform(Xsc_test)


from sklearn.svm import SVC


def credit_qsvm():
    nqubits = 4
    dev = qml.device('lightning.qubit', wires=nqubits)


    @qml.qnode(dev)
    def kernel_circ(a, b):
        qml.AmplitudeEmbedding(a, wires=range(nqubits), pad_with=0, normalize=True)
        qml.adjoint(qml.AmplitudeEmbedding(b, wires=range(nqubits), pad_with=0, normalize=True))
        return qml.probs(wires=range(nqubits))

    def qkernel(A, B):
        return np.array([[kernel_circ(a, b)[0] for b in B] for a in A])

    qsvm = SVC(kernel=qkernel).fit(Xlda_train, y_res)
    return 0


credit_qsvm()









